==========================================================================================
[강화학습 주식투자 추가 내용]

==> 우선 저자 깃허브의 코드와 코드 비교해볼 것 (내용 다른 부분이 조금 있는 듯 - 파이참으로 비교)

Tf로 바꾸기
네트워크 수정 및 추가

팩터 추가
매매 수수료 및 세금 추가
정책 신경망 출력 결과의 확률 중 max값을 선택해도 그 확률 자체가 낮을 수도 있으므로 일정 확률 이상의 값이 되어야만 그 max 값을 선택

지도 학습은 추세 학습 못하는 듯(확인) - 강화학습은?    비지도학습은?


self Q. 알파고가 스스로 자신(다른 알파고)와 경쟁하며 학습력을 향상시키듯이 RA 알고리즘(모델)도 서로 경쟁하며 발전할 수 있을까? (같은 알고리즘(모델)간 경쟁 & 다른 알고리즘(모델)간 경쟁)

("KDB대우증권 관계자는 “무궁무진한 퀀트베이스가 바탕”이라며 “알고리즘이 경쟁하는 플랫폼 구축을 통해 투자자가 자기성향에 맞는 알고리즘을 선택하고 자동으로 포트폴리오를 리벨런싱한다는 점에서 시스템트레이딩보다 진일보한 서비스”라고 설명했다. 한때 특허받은 ETF자동매매시스템인 스마트인베스터로 시스템트레이딩붐을 일으켰던 NH투자증권도 마찬가지. NH투자증권 관계자는 “시스템트레이딩 변형한 형태로 다양한 알고리즘을 통해 자기투자성향에 맞는 최적화된 자산배분이나 투자전략을 짤 수 있는 것이 장점”이라며 “포괄적인 자산관리의 툴로 확대하고 있다”라고 말했다. 한편 로보어드바이저의 성공가능성에 대해 기대와 우려가 엇갈린다. [http://www.fntimes.com/html/view.php?ud=141741]


selfQ. 이런저런 데이터와 알고리즘을 가져와서 예측을 한다고해도 시장의 Madness를 감안하지 않는 이상 더 이상 발전 불가하다고 생각 --> Madness를 측정하고 알고리즘에 반영할 수 있는 방법?

==========================================================================================



P.73: 에이전트 상태 다양화

P.74: 거래 횟수 변경 및 수수료와 세금 추가

P.74: 관망이 유리한 상황 계산에 대한 확인 필요

P.74: max_trading_unit - min_trading_unit이 2 이상이 되도록 설정할 것

P.75: 행동에 대한 확신과 지연 보상 임계치 변경 

P.76: 학습 단계에서 한 에포크마다 에이전트의 상태 초기화해야함 --> Why?

P.77: 투자 행동 결정에 영향을 주는 것이 (그래서 정책 신경망의 입력에 포함할만한 것이) 또 뭐가 있을까
	- 수익 혹은 손실이 몇 번 연속으로 발생했는지?

P.79: 신용 매수 및 공매도 고려

P.80: 확률에 따른 매수/매도 행동 단위 결정하는 식 (decide_trading_unit() 함수) 이해 불가 --> 해결했으나 다른 의문점 발생 (Q_1과 Q_1_2)

P.80: agent.py 코드의 confidence 쪽 TODO 확인 필수 --> confidence가 소프트맥스 확률 값이라면 두 행동에 대한 확률의 합은 1이 된다. 그렇다면 선택되는 행동의 값은 무조건 0.5 보다 높게 된다. --> 이럴 경우 위의 <<보완>>에서 작성한 선택된 확률 자체가 낮게 나오는 경우는 없게 될 것이다. --> Q. 그러나 이는 둘이 합쳐서 반드시 혹은 어쩔 수 없이 1이 나와야 하는 상황인데, 이 수치가 신뢰할 만한 수준인지 의문이 남는다. 확인 필수!

P.81: 좋은 식(알고리즘) --> 주석 보고 다시 이해해 볼 것 --> 그러나 다시 문제 발생

P.83: 포트폴리오 가치의 등락률이 0인 경우에도 긍정 보상을 주는 경우는 재고의 여지가 있다고 판단된다.

P.84: 정책 신경망 LSTM 구성을 더 다양하게 시도해볼 것

P.86: 정책 신경망에는 Loss Function이 따로 없다는 말이 있는데 무엇이 맞는가? (코드 참고 - policy_network.py)

P.87: 여러 학습 알고리즘 및 방법 적용해 볼 것

P.88: 코드 보면 왜 2차원으로 안하고 3차원으로 한건가? 그리고 괄호는 왜 2개야? --> 케라스에서 필요로하는 성질에 따른 것인가? (코드 참고 - policy_network.py)

P.94: 일봉차트 x축을 날짜로 바꿀 것

P.94: np.hstack은 왜 또 괄호가 두 개인가?

P.94: tolist()를 쓰는 이유는 무엇인가?

P.97: 43번줄 부터 시작하는 for문 이해 불가

P.97: axes[1]의 x축 범위가 어느정도인지 정한 적이 없는데 axvline에 바로 i만 넣어도 되나? --> 앞의 subplots에서 sharex를 true로 했으니 괜찮은 것인가? (아님 아예 지정을 안해도 되는건가?)

P.가시화기 부분: visualizer.py 부분이 논리/알고리즘은 이해되는데 input 데이터들이 어떤 모양인지 모르니 정확히 이해가 안됨 --> 코드 부분 완료 후 다시 이해해 볼 것 --> 그리고 코드의 TODO 부분들 확인해볼 것

P.99: learning은 특히 어떻게 구성되어있는 것인지 확인 필수

P.101: x축으로 들어가는 데이터에 휴일은 빠져있지 않나? 

P.104: PolicyLearner를 중심으로 코드 다시 한 번 이해 필수 --> 오래걸릴 것 그러나 전체 과정 이해를 위해 필수

P.107: (저장된 이미지에는 나오긴 하지만) 파일명에도 몇 번째 에포크인지 나타내는 것이 좋을 듯함

P.110: memory_sample, action, reward ... 등이 값는 값의 형태가 for문을 돌면서 값이 어떻게 쌓이는지 확인해볼 것 (value & shape)

P.110: 탐험 위치와 학습 위치가 필요한 이유는 무엇인가?

P.114: itr_cnt는 "수행한 에포크 수" 라더니 웬 인덱스?

P.121: 이렇게 호출만 할거면 environment 모듈에서 이 함수 return은 왜 한거야? (--> 여러모로 따져보았을 때, 이 책 코드의 완성 상태가 그렇게 좋은 것 같지는 않음)

P.:

P.:

P.:

P.:

P.:

P.:

P.:

P.:

P.:

P.:

P.:

P.:

P.:

P.:

P.:

P.:

P.:

P.:

P.:

P.:

P.:

P.:

P.:

P.:

P.:

P.:

P.:

P.:

P.:

<변수명 변경>

P.72: agent 모듈에서 현재 (현금) 잔고 변수인 balance를 --> curr_balance로 변경

P.72: agent 모듈에서 초기 자본금 설정 함수인 set_balance()를 --> set_initial_balance()로 변경

P.72: agent 모듈에서 에이전트 상태 획득 함수인 get_states()를 --> get_agent_states()로 변경

P.75: agent 모듈에서 주식 보유 비율 변수인 self.ratio_hold를 --> self.equity_holding_ratio로 변경

P.75: agent 모듈에서 포트폴리오 가치 비율 변수인 self.ratio_portfolio_value를 --> self.portfolio_value_ratio로 변경

P.78: agent 모듈에서 각 행동에 대한 확률 변수인 probs를 --> actions_probabilities로 변경

P.78: agent 모듈에서 선택된 행동에 대한 확률 변수인 confidence를 --> selected_action_probability로 변경

P.83: agent 모듈에서 포트폴리오 가치의 등락률 변수인 profitloss를 --> profit_loss_ratio로 변경

P.84: policy_network 모듈에서 agent 모듈에서 쓰이는 probs와 같은 용도로 사용되는 prob 변수를 --> probs로 그리고 이를 다시 --> probabilities로 변경

P.:

P.:

P.:

P.:


> chart_data: 주식 종목의 차트 데이터, 즉 특정 기업의 DOHLCV를 가지고 있는 데이터

> sample: 15개의 학습 데이터와 2개의 에이전트 상태 데이터가 합쳐져 17개의 속성을 갖는 정책 신경망 입력 데이터(훈련 데이터)

> Environment 클래스
    - 생성자에서 chart_data 사용
     - 생성자가 chart_data를 인자로 받아옴

    - 생성자에서 observation 정의 
     - observation에는 (observe 함수에 의해) chart_data의 특정 위치의 DOHLCV가 저장된다.
    
    - observe 함수 정의
     - chart_data에서 특정 위치의 DOHLCV를 가져온다.


> Agent 클래스
    - min_trading_unit, max_trading_unit가 agent 클래스의 생성자 인자값으로 들어간다.
    
    - delayed_reward_threshold가 agent 클래스의 생성자 인자값으로 들어간다. (손익률이 이 값을 넘으면 지연 보상이 발생한다.)

    - 

    - 

    - 

    - 지연 보상 임계치를 넘어야 기준 포트폴리오 가치(base_portfolio_value)의 갱신이 이루어진다.














