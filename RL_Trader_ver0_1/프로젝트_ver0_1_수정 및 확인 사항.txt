====================================================================================================================
[강화학습 주식투자 추가 내용]

==> 우선 저자 깃허브의 코드와 코드 비교해볼 것 (내용 다른 부분이 조금 있는 듯 - 파이참으로 비교)

Tf로 바꾸기
네트워크 수정 및 추가

팩터 추가
매매 수수료 및 세금 추가
정책 신경망 출력 결과의 확률 중 max값을 선택해도 그 확률 자체가 낮을 수도 있으므로 일정 확률 이상의 값이 되어야만 그 max 값을 선택

지도 학습은 추세 학습 못하는 듯(확인) - 강화학습은?    비지도학습은?


self Q. 알파고가 스스로 자신(다른 알파고)와 경쟁하며 학습력을 향상시키듯이 RA 알고리즘(모델)도 서로 경쟁하며 발전할 수 있을까? (같은 알고리즘(모델)간 경쟁 & 다른 알고리즘(모델)간 경쟁)

("KDB대우증권 관계자는 “무궁무진한 퀀트베이스가 바탕”이라며 “알고리즘이 경쟁하는 플랫폼 구축을 통해 투자자가 자기성향에 맞는 알고리즘을 선택하고 자동으로 포트폴리오를 리벨런싱한다는 점에서 시스템트레이딩보다 진일보한 서비스”라고 설명했다. 한때 특허받은 ETF자동매매시스템인 스마트인베스터로 시스템트레이딩붐을 일으켰던 NH투자증권도 마찬가지. NH투자증권 관계자는 “시스템트레이딩 변형한 형태로 다양한 알고리즘을 통해 자기투자성향에 맞는 최적화된 자산배분이나 투자전략을 짤 수 있는 것이 장점”이라며 “포괄적인 자산관리의 툴로 확대하고 있다”라고 말했다. 한편 로보어드바이저의 성공가능성에 대해 기대와 우려가 엇갈린다. [http://www.fntimes.com/html/view.php?ud=141741]


selfQ. 이런저런 데이터와 알고리즘을 가져와서 예측을 한다고해도 시장의 Madness를 감안하지 않는 이상 더 이상 발전 불가하다고 생각 --> Madness를 측정하고 알고리즘에 반영할 수 있는 방법?

====================================================================================================================


<여기 써있지 않은 코드의 TODO들도 해결해야한다.)

<<근본적 중요성: 이 질문에 대한 답을 내리지 못하면 강화학습 의미가 없어지게 된다. - policy_network.py 초입부분에 질문 있음>> 
==> 바로 위에서 언급한 "긍정적 행동을 진행 방향으로 잡고 목표 함수(maximize 긍정 지연 보상)를 최대화 하는 정책 경사의 역할" 이 policy_learner.py의 코드 중 어느 부분에 해당되는가? ==> (같은 질문) policy_learner.py 에서 epoch 마다 초기화가 진행되는데 학습한 내용은 초기화되지 않을 것이다. 그럼 이 학습한 내용을 담는 코드는 무엇인가?

<현재 관망은 매수를 결정했지만 매도가 불가능하고, 매도를 결정했지만 매도가 불가능한 경우에 발생한다. 즉 매수를 결정했는데 잔고가 부족하거나 매도를 결정했는데 보유 주식 잔고가 없는 경우에 수행하게 된다. --> 이렇게 어쩔 수 없는 외부 요인으로 인한 상황에 따라 관망을 하는 것이 아니라 정말로 관망을 하는 것이 최적의 선택이기 때문에 선택하게 되는 상황을 추가해볼 것.>



P.73: 에이전트 상태 다양화

P.74: 거래 횟수 변경 및 수수료와 세금 추가

P.74: 관망이 유리한 상황 계산에 대한 확인 필요

P.74: max_trading_unit - min_trading_unit이 2 이상이 되도록 설정할 것

P.75: 행동에 대한 확신과 지연 보상 임계치 변경 

P.76: 학습 단계에서 한 에포크마다 에이전트의 상태 초기화해야함 --> Why?

P.77: 투자 행동 결정에 영향을 주는 것이 (그래서 정책 신경망의 입력에 포함할만한 것이) 또 뭐가 있을까
	- 수익 혹은 손실이 몇 번 연속으로 발생했는지?
	- 에이전트가 연속으로 매수한 횟수
	- 에이전트가 연속으로 매도한 횟수
	- 20거래일 동안의 매수 비율
	- 20거래일 동안의 매도 비율 등

P.79: 신용 매수 및 공매도 고려

P.80: 확률에 따른 매수/매도 행동 단위 결정하는 식 (decide_trading_unit() 함수) 이해 불가 --> 해결했으나 다른 의문점 발생 (Q_1과 Q_1_2)

P.80: agent.py 코드의 confidence 쪽 TODO 확인 필수 --> confidence가 소프트맥스 확률 값이라면 두 행동에 대한 확률의 합은 1이 된다. 그렇다면 선택되는 행동의 값은 무조건 0.5 보다 높게 된다. --> 이럴 경우 위의 <<보완>>에서 작성한 선택된 확률 자체가 낮게 나오는 경우는 없게 될 것이다. --> Q. 그러나 이는 둘이 합쳐서 반드시 혹은 어쩔 수 없이 1이 나와야 하는 상황인데, 이 수치가 신뢰할 만한 수준인지 의문이 남는다. 확인 필수!

P.81: 좋은 식(알고리즘) --> 주석 보고 다시 이해해 볼 것 --> 그러나 다시 문제 발생

P.83: 포트폴리오 가치의 등락률이 0인 경우에도 긍정 보상을 주는 경우는 재고의 여지가 있다고 판단된다.

P.84: 정책 신경망 LSTM 구성을 더 다양하게 시도해볼 것

P.86: 정책 신경망에는 Loss Function이 따로 없다는 말이 있는데 무엇이 맞는가? (코드 참고 - policy_network.py)

P.87: 여러 학습 알고리즘 및 방법 적용해 볼 것

P.88: 코드 보면 왜 2차원으로 안하고 3차원으로 한건가? 그리고 괄호는 왜 2개야? --> 케라스에서 필요로하는 성질에 따른 것인가? (코드 참고 - policy_network.py)

P.94: 일봉차트 x축을 날짜로 바꿀 것

P.94: np.hstack은 왜 또 괄호가 두 개인가?

P.94: tolist()를 쓰는 이유는 무엇인가?

P.97: 43번줄 부터 시작하는 for문 이해 불가

P.97: axes[1]의 x축 범위가 어느정도인지 정한 적이 없는데 axvline에 바로 i만 넣어도 되나? --> 앞의 subplots에서 sharex를 true로 했으니 괜찮은 것인가? (아님 아예 지정을 안해도 되는건가?)

P.가시화기 부분: visualizer.py 부분이 논리/알고리즘은 이해되는데 input 데이터들이 어떤 모양인지 모르니 정확히 이해가 안됨 --> 코드 부분 완료 후 다시 이해해 볼 것 --> 그리고 코드의 TODO 부분들 확인해볼 것

P.99: learning은 특히 어떻게 구성되어있는 것인지 확인 필수

P.101: x축으로 들어가는 데이터에 휴일은 빠져있지 않나? 

P.104: PolicyLearner를 중심으로 코드 다시 한 번 이해 필수 --> 오래걸릴 것 그러나 전체 과정 이해를 위해 필수

P.107: (저장된 이미지에는 나오긴 하지만) 파일명에도 몇 번째 에포크인지 나타내는 것이 좋을 듯함

P.110: memory_sample, action, reward ... 등이 값는 값의 형태가 for문을 돌면서 값이 어떻게 쌓이는지 확인해볼 것 (value & shape)

P.110: 탐험 위치와 학습 위치가 필요한 이유는 무엇인가? 또한 이들의 의미는? (코드 참고 - policy_learner.py의 125번줄 TODO 확인)

P.114: itr_cnt는 "수행한 에포크 수" 라더니 웬 인덱스?

P.121: 이렇게 호출만 할거면 environment 모듈에서 이 함수 return은 왜 한거야? (--> 여러모로 따져보았을 때, 이 책 코드의 완성 상태가 그렇게 좋은 것 같지는 않음)

P.**: > chart_data: 주식 종목의 차트 데이터, 즉 특정 기업의 DOHLCV를 가지고 있는 데이터

      > sample: 15개의 학습 데이터와 2개의 에이전트 상태 데이터가 합쳐져 17개의 속성을 갖는 정책 신경망 입력 데이터(훈련 데이터)
        - training_data도 sample과 같은 형식이다.
          - 라고 하는데 정확히 sample & next_sample & training_data의 값과 형식이 무엇인지 파악해야한다!
          - policy_learner.py를 보면 "self.sample = self.training_data.iloc[self.training_data_idx].tolist()" 라는 코드가 있다.
          - 이에 따라 sample은 (policy_network.py에서 2차원으로 만들면) [1,15]이고, training_data는 이러한 sample의 모임, 즉 [-1,15]인 듯 하다. (에이전트의 상태 feature는 해당 코드 바로 밑에 "self.sample.extend(self.agent.get_states())"를 통해 추가되는 것이다. 따라서 training_data에서 나온 sample은 길이가 15가 맞고, 이 코드들을 가지는 _build_sample 함수의 반환값인 sample이 [1,17]이 되는 것이다.--> 확인 필요!

P.:

P.:

P.:

P.:

P.:

P.:

P.:

P.:

P.:

P.:

P.:

P.:

P.:

P.:

P.:

P.:

P.:

P.:

P.:

P.:

P.:

P.:

P.:

P.:

P.:

P.:

P.:

P.:

<변수명 변경>

P.72: agent 모듈에서 현재 (현금) 잔고 변수인 balance를 --> curr_balance로 변경

P.72: agent 모듈에서 초기 자본금 설정 함수인 set_balance()를 --> set_initial_balance()로 변경

P.72: agent 모듈에서 에이전트 상태 획득 함수인 get_states()를 --> get_agent_states()로 변경

P.75: agent 모듈에서 주식 보유 비율 변수인 self.ratio_hold를 --> self.equity_holding_ratio로 변경

P.75: agent 모듈에서 포트폴리오 가치 비율 변수인 self.ratio_portfolio_value를 --> self.portfolio_value_ratio로 변경

P.78: agent 모듈에서 각 행동에 대한 확률 변수인 probs를 --> actions_probabilities로 변경

P.78: agent 모듈에서 선택된 행동에 대한 확률 변수인 confidence를 --> selected_action_probability로 변경

P.83: agent 모듈에서 포트폴리오 가치의 등락률 변수인 profitloss를 --> profit_loss_ratio로 변경

P.84: policy_network 모듈에서 agent 모듈에서 쓰이는 probs와 같은 용도로 사용되는 prob 변수를 --> probs로 그리고 이를 다시 --> probabilities로 변경

P.96: visualizer 모듈에서 에이전트가 수항할 수 있는 전체 행동 리스트 변수인 action_list를 --> possible_action_list로 변경

P.96: visualizer 모듈에서 에이전트가 수행한 행동 배열 변수인 actions를 --> performed_actions로 변경

P.96: visualizer 모듈에서 정책 신경망의 출력 배열 변수인 outvals를 --> output_values로 변경

P.96: visualizer 모듈에서 탐험 여부 배열 변수인 exps를 --> explorations로 변경

P.96: visualizer 모듈에서 포트폴리오 가치 배열 변수인 pvs를 --> portfolio_values로 변경

P.96: visualizer 모듈에서 초기 자본금 배열 변수인 pvs_base를 --> base_portfolio_values로 변경

P.106: policy_learner 모듈에서 초기 투자 자본금 변수인 balance를 --> initial_balalnce로 변경 (+ fit 함수를 호출하는 main.py의 balance도 변경)

P.107: policy_learner 모듈에서 수익이 발생한 에포크 수를 저장하는 변수인 epoch_win_cnt를 --> wining_epoch_cnt로 변경

P.110: policy_learner 모듈에서 학습 위치 변수인 memory_learning_idx를 --> memory_learning_idx_val로 변경 (policy_learner.py의 TODO 확인)

P.119: (변수는 아니지만) policy_learner 모듈에서 Max PV는 --> Max Portfolio Value로 & Win은 --> Wining Epoch Count로 변경

P.:

P.:

P.:

P.:

P.:

P.:



> chart_data: 주식 종목의 차트 데이터, 즉 특정 기업의 DOHLCV를 가지고 있는 데이터

> sample: 15개의 학습 데이터와 2개의 에이전트 상태 데이터가 합쳐져 17개의 속성을 갖는 정책 신경망 입력 데이터(훈련 데이터)
    - training_data도 sample과 같은 형식이다.
       - 라고 하는데 정확히 sample & next_sample & training_data의 값과 형식이 무엇인지 파악해야한다!
       - policy_learner.py를 보면 "self.sample = self.training_data.iloc[self.training_data_idx].tolist()" 라는 코드가 있다.
       - 이에 따라 sample은 (policy_network.py에서 2차원으로 만들면) [1,15]이고, training_data는 이러한 sample의 모임, 즉 [-1,15]인 듯 하다. (에이전트의 상태 feature는 해당 코드 바로 밑에 "self.sample.extend(self.agent.get_states())"를 통해 추가되는 것이다. 따라서 training_data에서 나온 sample은 길이가 15가 맞고, 이 코드들을 가지는 _build_sample 함수의 반환값인 sample이 [1,17]이 되는 것이다.--> 확인 필요!

> Environment 클래스
    - 생성자에서 chart_data 사용
     - 생성자가 chart_data를 인자로 받아옴

    - 생성자에서 observation 정의 
     - observation에는 (observe 함수에 의해) chart_data의 특정 위치의 DOHLCV가 저장된다.
    
    - observe 함수 정의
     - chart_data에서 특정 위치의 DOHLCV를 가져온다.


> Agent 클래스
    - min_trading_unit, max_trading_unit가 agent 클래스의 생성자 인자값으로 들어간다.
    
    - delayed_reward_threshold가 agent 클래스의 생성자 인자값으로 들어간다. (손익률이 이 값을 넘으면 지연 보상이 발생한다.)

    - 

    - 

    - 

    - 지연 보상 임계치를 넘어야 기준 포트폴리오 가치(base_portfolio_value)의 갱신이 이루어진다.






dkkdkdkdkdkdkdk







